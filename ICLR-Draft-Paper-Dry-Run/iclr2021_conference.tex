\documentclass{article} % For LaTeX2e
\usepackage{iclr2021_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{caption}

% I got these from this guide:
% https://shantoroy.com/latex/how-to-write-algorithm-in-latex/
% Which I use to highlight how we do the experiment.
\usepackage{algorithm}
\usepackage{arevmath}
\usepackage[noend]{algpseudocode}

\usepackage{graphicx}
\graphicspath{ {.} }

\setcitestyle{square,comma}


\title{Exploring Neural Network Modularity \\ Using Model Stitching}

% Authors must not appear in the submitted version, you uncomment a line below
% which is labeled "iclrfinalcopy" to anonymize or not.
\author{Adriano Hernandez, Rumen Dangovski \& Peter Y. Lu \thanks{You can find our other work at \url{http://www.a14z.blog/}, \url{http://super-ms.mit.edu/rumen.html}, and \url{https://peterparity.github.io/} respectively.} \\
MIT EECS\\
Cambridge, MA 02139, USA \\
\texttt{\{adrianoh,rumenrd,lup\}@mit.edu}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}

\maketitle

\begin{abstract}
We expand \textit{model stitching} (Lenc \& Vedaldi 2015) as a methodology to compare neural networks.
Previously, Bansal, Nakkiran \& Barak used it to compare the representations learned by differently seeded
and/or trained neural networks.
We use it to compare the representations learned by neural networks with different architectures.
This enables us to reveal interesting behavior of model stitching. Namely, we find that stitching, 
based on convolutions, for small ResNets, can reach unexpectedly
high accuracy for intuitively different layers if those layers come later in the first (sender) network than in
the second (receiver). This leads us to hypothesize that stitches are not in fact learning to match the
representations expected by receiver layers, but instead to find different representations which nonetheless
yield similar results.
We do a simple numerical analysis to test this hypothesis and it yields mixed results.
\end{abstract}

\section{Introduction}
\label{Introduction}
The success of deep learning for visual recognition has been attributed to the ability of neural networks to learn
good representations of their training data \cite{Rumelhart1986LearningIR}. That is, intermediate outputs (which we refer
to as ``representations'') of good neural networks 
are believed to encode meaningful information about their inputs, which these neural networks use for classification and/or other
downstream machine learning tasks \cite{goodfellow2016deep}.
However, our understanding of these representations is somewhat limited. Though
deep learning interpretability research, particularly for computer vision, has helped us
to intuitively grasp what deep neural 
networks are learning, we do not
know why good representations are learned, nor do we have a robust theory to characterize them. For example, we do not
know how to compare representations effectively.

Being unable to compare representations is a fundamental limitation because it precludes us from understanding whether
different models are learning essentially the same thing. This is important, because one of the great hopes of the authors
of this paper is that
different neural networks are able to learn, by some measure, qualitatively similar representations, even if they are
numerically, or superficially, different. Metaphorically,
we would like to find that models learn ``the same sentences in different languages.''

Since two different representations can encode the same underlying information differently, one of the goals of this
paper is to advance our ability to ``translate these different languages,'' so to speak. Armed with such an ability,
if we found that indeed, most neural networks for certain
common tasks learn to encode the same
underlying information in their representations, that fact would lend credence to the belief that deep neural
networks, can learn meaningful concepts (if we can call representations ``concepts'')
like a human, since it is an unlikely coincidence that so many
different learners converge to similar representations.

At the very least this would be of philosophical interest and encourage explanatory theoretical research.
However, the ability to compare representations could empower us in more practical ways as well.
For example, we could use representational comparisons
to more rigorously test novel architectures based on older well-known ones. More generally, we could
use our existing knowledge about specific deep neural networks to gain more knowledge about others, quickly.
Thus, we want to be able to compare neural network representations.

\subsection*{Organization}
We first discuss related work in \hyperref[RelatedWork]{Section 2}. Subsequently we will provide a high-level view of our
contribution in \hyperref[Contribution]{Section 3} and explain our method of stitching in \hyperref[Stitching]{Section 4}.
In \hyperref[ExperimentalSetup]{Section 5} and \hyperref[Results]{Section 6} we will discuss our experimental setup and
results respectively, after which we wrap up with a conclusion in \hyperref[Conclusion]{Section 7}. Tables for one of
our numerical tests are available in the \hyperref[Appendix]{Appendix}.

\section{Related Work}
\label{RelatedWork}
Many non-learned similarity measures exist such as Centered Kernel Alignment \cite{Kornblith2019SimilarityON} and
Canonical Correlation Analysis \cite{Morcos2018InsightsOR}.

These may or may not be useful to explore the similarity of different representations. However, their efficacy relies
on the ingenuity of their designers since these measures are rather ad-hoc \cite{Bansal2021RevisitingMS}.
Recent research \cite{Ding2021GroundingRS} argues that some supposed improvements in these measures do not generalize.
Thus, we decide not to use these measures because results which use them their use are difficult to gague.
Instead, we choose a measure that depends entirely on accuracy: model stitching.

\section{Our Contribution}
\label{Contribution}
Recent advances in model stitching \cite{Bansal2021RevisitingMS} have made great strides towards the goal of effectively
comparing representations. Bansal et al. elaborate and use model stitching to confirm that architecturally similar
neural networks learn similar representations for computer vision.

They give us tantalizing results with positive practical by-products\footnote{
   They confirm that, on standard datasets, such as CIFAR-10, standard ResNets, such as ResNet18, are able to learn similar
   representations irrespective of their initial random seeds, layer widths, or (to a limited extent) training methodology.
   They use this finding to show that stitching wider neural networks into more narrow ones can improve performance. Moreover,
   they use stitching to tell how fast different layers converge to good solutions, which enables them to freeze
   fast-converging layers during training to save compute.
}.
In doing so open up a space of possibility for future research, and we see two main directions to explore.

\textbf{Different Architectures and Layer Shapes.} Bansal et al. only confirm that networks with the same 
fundamental architecture have similar representations at
the exact same layers. It is trickier to compare the representations of neural networks with different architectures
since they have different shapes, but in some respects this is the more meaningful and interesting thing to do.

\textbf{Sanity Testing Stitching.} Bansal et al. ignore a subtle point regarding the ability 
of stitching to compare representations. Model stitching essentially checks whether two representations
are interchangeable with minor modifications, using a \textit{learned} transformation to learn the best
such ``modifications.'' However, the transformation, which we call the stitch, is trained using backpropagation
from a downstream task like classification. It may not be necessary that two representations be ``similar,'' either
numerically or intuitively, for the stitched model to reach high task accuracy.

Our paper seeks to tackle these two questions. Firstly, we extend stitching in a simple way to be able to compare
different layers of off-the-shelf computer vision models. Secondly, we analyse 
whether different neural networks learn similar
representations. Lastly, we analyse whether stitching is a good metric in these cases using a simple numerical test. We
find that our simple extension of stitching yields unexpectedly high similarity for layers we thought should be different.
This result holds true, empirically, for all small simple ResNets \cite{He2016DeepRL} on CIFAR-10 \cite{CIFAR10},
varied by their random seed. This motivates
us to understand whether our stitches are actually behaving in the way we expect numerically. Our numerical test is not
fully conclusive, but suggests that not all stitches behave as expected.

\section{Stitching}
\label{Stitching}
We broaden the definition of stitching very little from that as defined in \cite{Bansal2021RevisitingMS}. In that paper,
stitching involved taking the intermediate output of some layer, which we call the sender, transforming it, and then
inputting that into an intermediate input in another network. We call the layer we input into the receiver. We
will also refer to the two networks, respectively, as sender and receiver when convenient and contextually clear.
The representation that would normally be recieved by the receiver we call the expected representation
\footnote{The expected representation is sent by
the expected sender: the previous layer in the receiver network.}.

The layers up to the sender (inclusive) we call a prefix and the layers from the receiver to the end (inclusive) we call
a suffix. The transformation used is called the stitch, and it is learned\footnote{We use gradient descent for simplicity.}
on a frozen\footnote{``Frozen'' refers to the fact that the weights are not updated during the backwards pass.}
prefix/suffix pair. The network formed by concatenating the sender prefix, the stitch, and the receiver
suffix, is called the stitched network, portrayed in \hyperref[Figure1]{Figure 1}. Just like Bansal et al., we use the task accuracy of the stitched network as our
primary training task.

The stitch belongs to a simple function class such as 1x1 convolutions (depthwise).
Bansal et al. pick these function classes since, if the sender and receiver networks are in the same
function class, the stitched network will also be in that same function class. However, since for us the sender and
receiver networks are not in the same function class, we allow stitches that modify the function class in minor ways.
We expand the class of stitches to include strided convolutions
\footnote{Our convolutions have stride equal to kernel width, so as to downsample
by some constant factor.} as well as 1x1 convolutions of nearest-upsampled layers
\footnote{Nearest-upsampled means that each pixel
is copied a constant number of times, corresponding to a square grid of identical pixels}.

\label{Figure1}
\begin{center}
   \begin{figure}[h!]
      \centering
      \caption{Stitching}
      \includegraphics[width=12cm]{stitch.jpg}
      \caption*{This diagram gives us a simple view of a stitched network. The black arrows denote the flow of data, while
      the grey arrows denote the flow of data that would have been present, had the two networks been used without stitching.
      The different blocks denote different layers (numbered) and the different colors denote different neural networks. The arrow
      from layer 3 of the blue network to layer 4 of the red network involves a simple stitch transformation, though it is left
      out of the image for concision. The boxed layers, plus the stitch, make up the stitched network. In this specific case,
      the stitched network's task accuracy asks as our similarity measure for red layer 3 with 
      blue layer 3, since red layer 4 expects the input to come from red layer 3.}
   \end{figure}
\end{center}

\section{Experimental Setup}
\label{ExperimentalSetup}
\subsection*{Models and Dataset}
We use the CIFAR-10 dataset with small ResNets. We chose CIFAR-10 because it is a small, well-understood dataset
of small RGB-images that we can train fast on. We chose ResNets because they were used by Bansal et al., giving our
results more context.

We compare all different
layers of ResNet34 and all
ResNets with between 10 and 18 layers. These small ResNets we characterize with 4-tuples, where 
each element is either 1 or 2, representing the number of residual blocks per blockset\footnote{
   Residual blocks are partitioned into sets of consecutive within which they have the same shape.
} out of the 4 blocksets
present in a ResNet.\footnote{
   Though within each block there are two layers,
   we focus on stitching out of and into blocks for simplicity.
} Since we never use 10 or more blocks per blockset, we can denote these 4-tuples unambiguously\footnote{
   If we were to refer to them by their numbers of layers there would be ambiguity, since the neural networks
   r1122 and r2211 have the same number of layers, but different shapes per layer if we were to map their
   layers one to one.
} as
r1111, r1112, and so on. This set of 16 ResNets\footnote{
   Since each element of the 4-tuple is either 1 or 2, there are 16 total choices.
} we refer to as the small ResNets.

Our small ResNets' representations double in depth and halve in both width and height every blockset.
The first depth is 64 and the first width and height
are both 32. Because the input dimensions have a depth of 3, and a width and height both of 32, ResNets have an
initial convolutional layer before the first blockset. Because the output dimension is a 10-dimensional vector,
for classification, they also have a fully connected layer at the very end.

\subsection*{Experiment}
We train ResNet34 as well as each possible small resnet on CIFAR, yielding an accuracy above 90\%. Because 
there are 16 such possible
small ResNets and we stitch every possible ordered pair\footnote{
   For example, stitching r1111 with r2222.
}, there are around 256 such pairs we stitch amongst the small
ResNets. In addition, we stitch ResNet34 to ResNet18 and vice versa to see whether our findings generalize to a
bigger network. In every ordered pair of networks being stitched, the former is the sender, and the latter
is the receiver. For any sender/receiver pair, we stitch from all layers of the sender,
excluding both any inside the residual blocks and the final fully connected layer, into all layers of the receiver
excluding both the convolution and any layers inside the residual blocks. 
Note that we do stitch out of and into all blocks per blockset if there is more than one.

We use randomly initialized ResNets as controls. Unlike Bansal et al., we only vary our neural networks by their
initialization weights. Otherwise our setup is nearly identical.
For every pair of networks that are stitched, we also create a random sender and a random
receiver both with the same architecture as the sender and receiver respectively. Just as the sender is stitched
into the receiver, the random sender is stitched into both the receiver and the random receiver, and the sender
is also stitched into the random receiver. This allows us to acertain that the stitches
are not overly powerful \cite{Bansal2021RevisitingMS}. The stitches are trained on
the standard CIFAR-10 classification task by using backpropagation through the stitched network.

We train each stitch for 4 epochs. This hyperparameter was selected after experimentation. For both
regular training of the network and training of the stitches we use stochastic gradient descent with momentum 0.9,
batch size 256, weight decay 0.01, learning rate 0.01, and a post-warmup cosine learning rate scheduler.
These hyperparameters were also arrived at from experimentation. The code will be available on Github.

For ordered pair of networks, we plot the accuracy of all the stitches between their prefixes and suffixes
on a grid based on the sending layer and the receiver layer. The layer is
denoted by an integer which counts how many layers came before it\footnote{
   The initial convolution is ``0,'' the first block of the first blockset is ``1,'' and so on.
}.

\subsection*{Numerical Testing}
Because one of our goals is to see whether stitches emulate the receiver's expected representation, we
also measure the final pointwise mean squared difference between representations generated by stitch and
those that are expected. This mean squared difference is a crude numerical method, 
but gives us a sense of whether the representations
the stitch generates are close to the expected ones, since if they are close, the difference ought to be very close
to zero. We use random networks for to give us a sense what a high mean squared difference
looks like, numerically.

Additionally, we use a ``similarity'' trained stitch to give us a sense of what a low mean
squared difference looks like. The similarity trained stitch is not trained using backpropagation
from the classification task, but instead on the mean squared loss corresponding to the mean squared difference.
It gives us a sense of what mean squared difference the stitch could reach if it truly were ``doing its best''
to emulate the expected representation. We found that we had to train these other stitches for around 30 epochs, and
at the end compared their mean squared difference not only with the expected representation but also with that
from the vanilla stitch.

We calculated basic statistics on the these mean squared differences. We calculated the average, standard deviation, minimum,
and maximum values over all network pairs seperated by whether those pairs were both original networks, or random, or some
combination thereof. For each network pair we simply averaged the mean squared difference for each representation.
We also took the same statistics for the diagonals of pairs of networks whose architectures were the
same to give us a baseline for what mean squared difference could have corresponded to Bansal et al.'s
experiment.

\section{Results}
\label{Results}
All our experimental results are formatted as layer to
layer similarity matrices. The row denotes the sender, while the column denotes the receiver.
The value in each entry is the accuracy of the stitched network formed by stitching that sender
and reciever. We interpret it as a similarity.
The expected sender is always one column to the left of the reciever, so we interpret each similarity
to corespond to its row and the column before it. By convention, layers which were not possible to stitch
(mainly due to their shape being too different) were assigned a similarity of 0.

\subsection*{Expectations}
For stitched network accuracy stitch matrices we were expecting to see some sort of diagonal
different non-random networks. Bansal et al.'s findings suggested that there would be a one-to-one layer correspondence between
two networks of the same architecture. That is to say, each layer on the sender could be stitched with maximum
accuracy only to a single layer on the reciever.

For the similarity trained stitch we had no real expectation regarding accuracy, but we did expect it to reach a similar, but
slightly lower, mean squared difference with the representation than the vanilla stitch. We also expected the mean squared
difference statistics for the diagonals to exhibit a lower mean and standard deviation, since we know from Bansal et al. that
those layers should be quite similar.

For our controls we expected to see low stitching accuracy throughout the board since the networks are random, though
it would also make sense if some of the earlier or later layers exhibited higher accuracy since the complexity of the stitch
may be enough to approximate those layers.

\subsection*{Vanilla Stitches}
We have found unexpected patterns in the stitching network accuracy.
When the sender is stitched into
the receiver and neither is random, all layers in the lower left hand triangle\footnote{
   The lower left hand traingle corresponds to the stitches whose entry's row is less than its column.
}
of the similarity matrix exhibit high stitched network accuracy. Note that, similarly in Bansal et al.,
we consider the stitched network accuracy to be high if it is nearly identical to that of the
(least accurate) original network\footnote{
   Whether we pick the least accurate does not particularly matter since accuracies tend to cluster heavily around 92-95\%.
}.

For random networks we saw what we expected.
When two random networks are stitched, nearly all stitched networks have low
accuracy with a rather uniform distribution. When
a sender is stitched with a random receiver, only stitched networks with stitches from the
later layers of the sender into the later layers of the (random)
receiver have high accuracy. When a random sender is stitched
into a receiver, only stitched networks with stitches into the early layers of the
receiver have high accuracy.

Examples are depicted below. The triangular pattern is present in \hyperref[Figure2]{Figure 2} and in \hyperref[Figure3]{Figure 3}.
The expected (and actual) mostly random pattern expected from our controls is present in \hyperref[Figure4]{Figure 4}.

\label{Figure2}
\begin{center}
   \begin{figure}[h!]
      \centering
      \caption{Triangle Pattern in Small ResNets}
      \includegraphics[width=6.5cm]{resnet1111_1111.png}
      \includegraphics[width=6.5cm]{resnet1112_1122.png}
      \caption*{Similarity matrix from sender to reciever.}
   \end{figure}
\end{center}

\label{Figure3}
\begin{center}
   \begin{figure}[h!]
      \centering
      \caption{Triangular Pattern in Larger ResNets}
      \includegraphics[width=9.5cm]{resnet18_resnet34_sims.png}
      \caption*{Similarity matrix from sender to receiver.}
   \end{figure}
\end{center}

\label{Figure4}
\begin{center}
   \begin{figure}[h!]
      \centering
      \caption{Mostly Random (Uniform) Pattern for Random Senders}
      \includegraphics[width=8.5cm]{resnet18_rand_resnet18_sims.png}
      \caption*{Similarity matrix from random ResNet18 to trained ResNet18.}
   \end{figure}
\end{center}

\subsection*{Similarity-Trained-Trained Stitch}
While the main purpose of the similarity trained stitches was to give our numerical estimates context,
we did, interestingly, find that they performed spectacularly badly on the downstream task. Despite training
for almost 8 times the number of epochs, the similarity-trained stitch task accuracy was nearly identical
to random, with the exception of the first layer, in which it was only marginally better. A depiction
is available on \hyperref[Figure5]{Figure 5}.

This suggests that naively emulating the expected representation is not enough to yield high task accuracy.
To put it concisely, numerical similarity is by far not enough for semantic similarity. Most likely,
there are a few number of subspaces of the representation space that matter much more than others, and the similarity
trained stitch is unable to weigh them as heavily as it should without any task information.

\label{Figure5}
\begin{center}
   \begin{figure}[h!]
      \centering
      \caption{Naive Similarity-Trained Training Tends To Fail}
      \includegraphics[width=7.5cm]{resnet1111_1111_autoencoder.png}
      \caption*{Similarity matrix from sender to receiver.}
   \end{figure}
\end{center}

\subsection*{Numerical Analysis}
We found that the vanilla stitches' representations did reach low mean squared difference with respect to the controls,
but were not able to reach the same average mean squared difference as the similarity-trained stitches. When considering
only the diagonals the average mean squared difference was marginally lower. The full numbers are available in the
\hyperref[Appendix]{Appendix}.

\textbf{Diagonals.} For the diagonals of pairs of vanilla networks, the average mean squared difference between
the stitch generated representation and that which was expected was 4.41e-2, with a standard deviation of
6.10e-2, while for random networks the averages were 3.25e-1 for random to trained, 1.26e2 for random to random, and
4.28e5 for trained to random with standard deviations in the same orders of magnitude. We found that similarity
trained stitches tended to be around half an order of magnitude smaller on average, with a standard deviation also
around half an order of magnitude smaller. We found that the similarity trained stitches' representations to be
around as different from the vanilla stitches' representations as they were from the expected representations.

\textbf{All Stitches.}. When we explored the same statistics on the set of all stitch-generated representations we found
similar orders of magnitude for mean squared differences between representations generated by stitches trained on
the original networks, but larger orders of magnitude when random networks were involved.

\subsection*{Interpretation}
The most interesting aspect of our results is the high accuracy of the stitching network
for layers in the lower left hand triangle. Given that we interpret the stitching network accuracy
as a similarity, our results suggest that each sender representation is similar with \textit{all}
the receiver representations from a layer before it. We expected to see that each layer would be
similar to 1 to 3 layers at most because the standard narrative in the research community is that 
every layer loses some amount of granular information, and so that information should not be reconstructable
in a \textit{interchangeability} test like stitching.

We see two main explanations for the results. The first is that the common narrative could be wrong and some neural
networks may in fact be able to maintain most if not all of the granular information of the image throughout their
processing of it. The second is that the stitch may be able to give the reciever a representation which, despite being
different from that which is expected, nonetheless yields high accuracy. I call this latter option ``hacking'' since it
is as if the stitch ``hacked'' the reciever.

The former option is very interesting because it upheaves our understanding of neural networks, but we decided to
examine the latter option instead, since stitching is a novel methodology for measuring representational
similarity, and unexpected behavior associated with it more likely stems from our lack of understanding or misapplication
of stitching and not some widespread false narrative regarding neural networks.

We mainly examine it with the mean squared difference statistics. A high mean squared difference would
imply that the stitch is different, while a low mean squared difference would imply that the stitch is similar. From our
findings in the \hyperref[Results]{Results} Section above, we infer that while stitches are usually able to approximate
the expected representation pretty reasonably, they might not always do so. If the average mean squared difference
had been many orders of magnitude larger than that for the similarity trained stitch, then we could have posited such
a result with more certainty, but as it stands it could be either way.

However, we do also observe that the similarity trained stitches' stitched networks' accuracies were always as 
bad as a random network, which tells us that simple numerical emulation of the expected representation is not
strongly related to task accuracy. If the the vanilla stitches were truly learning to emulate the expected representation,
we would expect to find higher accuracy for the similarity trained stitches' stitched networks, since they too are
learning to emulate it. For this reason we posit that stitches do not necessarily emulate the expected representation.

\section{Conclusion}
\label{Conclusion}
It appears that stitching does not behave for different architectures exactly as we had hoped, though it does not seem
to be generating representations altogether different from those that were expected. Bansal et al. found that stitching
is able to reach high accuracy for layers that correspond on pairs of identically architected networks. However, we find
that when comparing all layers of similar networks, later layers of the sender are similar to early layers of the sender
even though this was unexpected to us.

We hypothesize that in some cases
stitches may be able to ``hack'' the receiver by learning to give it novel representations which nonetheless yield high
accuracy. However, it appears that stitches do not deviate, numerically, that much from the expected representation on
average. Most likely, there are subspaces in the representation space more important than others and the vanilla stitch
is able to optimize them, potentially at the expense of others, thus yielding slightly higher mean squared difference with
respect to the expected representation than a similarity trained stitch, even though it reaches much higher accuracy.

\section*{Acknowledgments}
\label{Acknowledgments}
Thank you to the SuperUROP benefactors (MIT EECS) for funding this project as well as my mentors, and the
SuperUROP faculty and staff for helping me embark on this research and write this paper.
\newpage
\bibliography{iclr2021_conference}
\bibliographystyle{iclr2021_conference}
\newpage
\appendix
\section{Appendix}
\label{Appendix}
Below are numerical talbes for the mean squared differences between different representations
after training with different stitching methods. Vanilla stitches were trained using backpropagation
through the stitched network from the CIFAR-10 classification task. Similarity-Trained stitches were
trained using a mean squared error loss with respect to the expected representation of the
receiver layer.

We compile tables of simple statistics for the mean squared differences both for all the stitches as well
as for the diagonals, which we expect to tend to exhibit smaller mean squared differences. For each comparison
we use stitches to and from randomly weighted models as controls.

\subsection*{Diagonals: Vanilla Stitches}
\label{Table1}
\begin{tabular}{c c c c c}
   Model Pair&Minimum&Mean&Maximum&Standard Deviation\\
   Trained to Trained&2.03e-03&4.41e-02&2.80e-01&6.10e-02\\
   Trained to Random&1.27e-01&4.28e+05&4.24e+06&1.12e+06\\
   Random to Trained&1.50e-02&3.25e-01&5.29e-01&1.56e-01\\
   Random to Random&1.58e-01&1.26e+02&1.36e+03&2.92e+02\\
\end{tabular}

\label{Table2}
\subsection*{Diagonals: Similarity-Trained Stitches}
\begin{tabular}{c c c c c}
   Model Pair&Minimum&Mean&Maximum&Standard Deviation\\
   Trained to Trained&2.15e-05&1.50e-02&1.91e-01&3.73e-02\\
   Trained to Random&5.52e-02&7.60e+04&3.56e+05&1.26e+05\\
   Random to Trained&3.48e-05&1.10e-03&5.91e-03&2.01e-03\\
   Random to Random&1.73e-02&4.94e+00&6.03e+01&1.27e+01\\
\end{tabular}

\label{Table3}
\subsection*{Diagonals: Similarity-Trained Stitches vs. Vanilla Stitches}
\begin{tabular}{c c c c c}
   Model Pair&Minimum&Mean&Maximum&Standard Deviation\\
   Trained to Trained&1.21e-03&2.33e-02&1.14e-01&3.89e-02\\
   Trained to Random&2.71e-03&1.67e+05&3.67e+06&7.32e+05\\
   Random to Trained&7.94e-03&3.23e-01&5.29e-01&1.59e-01\\
   Random to Random&1.17e-01&1.18e+02&1.35e+03&2.86e+02\\
\end{tabular}

\label{Table4}
\subsection*{All Representation Pairs: Vanilla Stitches}
\begin{tabular}{c c c c c}
   Model Pair&Minimum&Mean&Maximum&Standard Deviation\\
   Trained to Trained&1.63e-03&4.49e-02&3.63e+00&1.37e-01\\
   Trained to Random&1.55e-04&2.63e+05&1.78e+07&1.00e+06\\
   Random to Trained&1.35e-02&2.21e+00&2.27e+02&1.17e+01\\
   Random to Random&1.34e-01&7.25e+04&1.24e+07&5.94e+05\\
\end{tabular}

\label{Table5}
\subsection*{All Representation Pairs: Similarity-Trained Stitches}
\begin{tabular}{c c c c c}
   Model Pair&Minimum&Mean&Maximum&Standard Deviation\\
   Trained to Trained&5.39e-06&1.58e-02&1.32e+00&5.90e-02\\
   Trained to Random&1.71e-07&1.06e+05&9.19e+06&4.27e+05\\
   Random to Trained&6.32e-06&2.05e-03&1.88e-02&4.31e-03\\
   Random to Random&1.59e-02&4.20e+04&6.90e+06&3.44e+05\\
\end{tabular}

\label{Table6}
\subsection*{All Representation Pairs: Similarity-Trained Stitches vs. Vanilla Stitches}
\begin{tabular}{c c c c c}
   Model Pair&Minimum&Mean&Maximum&Standard Deviation\\
   Trained to Trained&7.83e-04&1.98e-02&5.87e-01&3.25e-02\\
   Trained to Random&1.54e-04&6.26e+04&5.79e+06&3.43e+05\\
   Random to Trained&7.94e-03&2.21e+00&2.27e+02&1.17e+01\\
   Random to Random&9.44e-02&2.12e+04&4.77e+06&2.08e+05\\
\end{tabular}

\end{document}
