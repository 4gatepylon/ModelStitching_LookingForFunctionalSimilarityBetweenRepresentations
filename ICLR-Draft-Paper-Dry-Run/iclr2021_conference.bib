@book{goodfellow2016deep,
  title     = {Deep learning},
  author    = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
  volume    = {1},
  year      = {2016},
  publisher = {MIT Press}
}

@inproceedings{Rumelhart1986LearningIR,
  title  = {Learning internal representations by error propagation},
  author = {David E. Rumelhart and Geoffrey E. Hinton and Ronald J. Williams},
  year   = {1986}
}

@inproceedings{Bansal2021RevisitingMS,
  title     = {Revisiting Model Stitching to Compare Neural Representations},
  author    = {Yamini Bansal and Preetum Nakkiran and Boaz Barak},
  booktitle = {NeurIPS},
  year      = {2021}
}

@article{Ding2021GroundingRS,
  title   = {Grounding Representation Similarity with Statistical Testing},
  author  = {Frances Ding and Jean-Stanislas Denain and Jacob Steinhardt},
  journal = {ArXiv},
  year    = {2021},
  volume  = {abs/2108.01661}
}

@article{Kornblith2019SimilarityON,
  title   = {Similarity of Neural Network Representations Revisited},
  author  = {Simon Kornblith and Mohammad Norouzi and Honglak Lee and Geoffrey E. Hinton},
  journal = {ArXiv},
  year    = {2019},
  volume  = {abs/1905.00414}
}

@inproceedings{Morcos2018InsightsOR,
  title     = {Insights on representational similarity in neural networks with canonical correlation},
  author    = {Ari S. Morcos and Maithra Raghu and Samy Bengio},
  booktitle = {NeurIPS},
  year      = {2018}
}

@article{He2016DeepRL,
  title   = {Deep Residual Learning for Image Recognition},
  author  = {Kaiming He and X. Zhang and Shaoqing Ren and Jian Sun},
  journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year    = {2016},
  pages   = {770-778}
}
@article{CIFAR10,
  title    = {CIFAR-10 (Canadian Institute for Advanced Research)},
  journal  = {},
  author   = {Alex Krizhevsky and Vinod Nair and Geoffrey Hinton},
  year     = {},
  url      = {http://www.cs.toronto.edu/~kriz/cifar.html},
  abstract = {The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. 
              
              The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class. },
  keywords = {Dataset},
  terms    = {}
}

@article{https://doi.org/10.48550/arxiv.1803.03635,
  doi = {10.48550/ARXIV.1803.03635},
  
  url = {https://arxiv.org/abs/1803.03635},
  
  author = {Frankle, Jonathan and Carbin, Michael},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}