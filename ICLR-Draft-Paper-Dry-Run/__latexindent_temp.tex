\documentclass{article} % For LaTeX2e
\usepackage{iclr2021_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{caption}

% I got these from this guide:
% https://shantoroy.com/latex/how-to-write-algorithm-in-latex/
% Which I use to highlight how we do the experiment.
\usepackage{algorithm}
\usepackage{arevmath}
\usepackage[noend]{algpseudocode}

\usepackage{graphicx}
\graphicspath{ {.} }


\title{Exploring Neural Network Modularity \\ Using Model Stitching}

% Authors must not appear in the submitted version, you uncomment a line below
% which is labeled "iclrfinalcopy" to anonymize or not.
\author{Adriano Hernandez, Rumen Dangovski \& Peter Y. Lu \thanks{You can find our other work at \url{http://www.a14z.blog/}, \url{http://super-ms.mit.edu/rumen.html}, and \url{https://peterparity.github.io/} respectively.} \\
MIT EECS\\
Cambridge, MA 02139, USA \\
\texttt{\{adrianoh,rumenrd,lup\}@mit.edu}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
We expand \textit{model stitching} (Lenc \& Vedaldi 2015) as a methodology to compare neural networks.
Previously, Bansal, Nakkiran \& Barak used it to compare the representations learned by differently seeded and/or trained neural networks.
We use it to compare the representations learned by neural networks with different architectures.
This gives us insight into the properties of stitching. Namely, we find that stitching can reach unintuitively
high accuracy for intuitively different layers if those layers come earlier in the first (sender) network than in
the second (reciever). This leads us to hypothesize that stitches are not in fact learning to match the
representations expected by reciever layers, but instead to find representations which yield similar results.
Exploration on this front is ongoing.
% Previously we wanted to do modular structure.
\end{abstract}


%
%
% Everything below this line
% is the original ICLR template.
% I keep it here specifically for
% the purpose of reminding me of how to do various things...
% I need to figure out how to get citations in there...
% 
% Papers to cite:
% https://arxiv.org/abs/2106.07682 (Revisiting Model Stitching)
%
%
%
%
%%
% https://arxiv.org/abs/2108.01661 (Grounding Representation Similarity with Statistical Testing)
%
% Start here: https://bair.berkeley.edu/blog/2021/11/08/similarity/.
% From this one they basically are trying to create a benchmark to test if a test of similarity is
% good or not. So for example you give them something like CKA or CCA and then they find whether it's
% a good measure or not. However, this "good measure" measure is for A SPECIFIC TASK. So, they are testing
% if FOR A SPECIFIC TASK the measure of similarity is able to discern differences (where accuracy is used
% to tell if the two models are actually different). I'm guessing it's rather harder to use just accuracy
% because it's unclear how you should compare specific layers. They use linear probes (linear classifiers
% trained on top of the intermediate layers we want to compare). They also use accuracy of the whole
% network (though unless you isolate the specific section you change in the network).
%
% Sensitivity: meaningful change is made to a representation => the similarity measure should reflect it.
% Specificity: non-meaningful change is made to a representation => similarity does not change much.
%
% Principle components refers to the principle components as defined by Principle Component Analysis (PCA)
% Read: https://en.wikipedia.org/wiki/Principal_component_analysis. The most informative components
% are the largest ones according to PCA (or at least they can be sorted that way).
%
% It was hard for me to understand what the similarity measures were doing. I need to take a statistics class.
% I also need to review, probably, some aspects of linear algebra or optimization.
%
% I did not totally finish this paper since it was hard to read for me. I will want to write or speak about
% what they are measuring. Here (below) is an outline:
% 1. Definitions of similarity metrics f (OOD and/or linear probe accuracy).
% 2. Definitions of models S (NLP and Vision) and how they were trained
% 3. Descriptions of intuitive tests. We run a test and measure f over it.
%   a. Sensitivity: 
%   b. Specificity: 
% 4. Description of a more rigorous operation
%  a. Definition: pick f (a function from the representation to a real number) as our "accuracy metric"
%  and S as our set of representations to measure similarity. Pick an A inside S (probably the one from the
%  the model with the highest accuracy, or just pick arbitrarily). For all B in S (including cases where
%  A = B) find d(A, B) using the similarity or dissimilarity metric "d" which we are testing, and also find
%  |f(A) - f(B)| which is the variation in "accuracy." Then find Corr(d(A, B), |f(A) - f(B)|) where Corr is
%  a measure of correlation. They use "Tau" and "Rho" which are the Kendall and Spearman measures of
%  correlation, respectively.
%  b. Benchmark 1: layer depth (i.e. which layer).
%  c. Benchmark 2: principle component deletion.
%  d. Pre-training Seed and/or Fine-tuning Seed. For the layer depths the pre-training seed is also varied.
%  For vision the training seed is used (they are Res-Net-14's trained from scratch). For vision no tests seem
%  to be done for PWCCA. I did not read why.
%
% Things that remain to be understood by me:
% 1. The specifics of their experiments (the models, the data, etcetera).
% 2. The definitions of correlation and whether a better "Corr" function might exist.
% 3. The relation between the Wikipedia definitions of "sensitivity" and "specificity" and the usage
%  of those words in the paper (I think it's just meant colloquially, but it's not totally clear).
% 4. The computation and meaning of principle components.
% 5. The meaning and computation of the different similarity metrics. Not only did I not understand them
%  very well, at times I did not even parse them properly.

%%
%% From Revisiting Model Stitching %%
% Understanding intermediate layers using linear classifier probes
% Learning internal representations by error propagation
% Towards understanding learning representations: To what extent do different neural networks learn the same representation, 2018
% Do wide and deep networks learn the same things? uncovering how neural network representations vary with width and depth
% Convergent learning: Do different neural networks learn the same representations?
% Bad global minima exist and sgd can reach them. 
% Insights on representational similarity in neural networks with canonical correlation
% Similarity of neural network representations revisited
% Deep residual learning for image recognition
% Linear mode connectivity and the lottery ticket hypothesis
% On the surprising similarities between supervised and self-supervised models
% Multimodal neurons in artificial neural networks.
%
%

% There is a strict upper limit to 8 pages
% Do NOT refer to the line numbers it automatically generates in your paper
% 
% 
% 
% \section{Introduction Guide}
% ... Specifically, say what your goal is (concretely) on a high level as well as explain your journey
% (help them step in your shoes and see things how you see them).

% Statsis freewrite:
% \begin{itemize}
%    \item What facts am I responding to? (social trends; discplinary trends; etc.)
%    \item What are the consequences of those facts?
%    \item What is valuable from a researcher/societal perspective about existing facts/work?
%    \item How do those consequences lead to an open question?
% \end{itemize}

% The facts that I am responding to are a few. Firstly, we do not know how to properly compare representations.
% This stems also from the fact that we do not understand what a lot of different types of neural networks are
% doing (i.e. we understand some convolutional neural networks but not totally, and attention networks are a lot
% more black-boxy). The consequences are that we find it hard to create robust and human-like-feature-generating
% models especially when using non-CNNs. Also, perhaps, we can care about the fact that training is expensive and
% that we cannot really reuse weights effectively. Lastly, we need to do a lot of data preprocessing, postprocessing,
% etcetera to be able to turn our model's expected inputs/outputs into something that can be used in real life. Thus,
% if we want to do full neural computation we need to break it down and understand it in more detail. The value
% element for our research is that we enble future research to use our building blocks to create tangible things
% (basically we build intellectual infrastructure). These consequences lead to open questions on a high level:
% how can we compare representations? How can we reuse weights most effectively? How can we understand if a
% model learns a human-understandable representation? How can we best combine different models?

% \begin{itemize}
%    \item What have others done in response to this same set of facts or in response to this question?
%    \item What did they find?
%    \item What motivates their work?
% \end{itemize}

% The stitching paper has done what I'm doing but in a more limited sense. They found that neural networks that we expect
% to be easy to compare and learn similar things do in fact meet those conditions. Their own work was motivated by the same
% curiosity which motivates mine, though it was primarily focused on how we can compare representations.

% \begin{itemize}
%    \item What do you do in response to this question?
%    \item What challenges exist for your project?
%    \item How do you set up your approach (what must you define in order for it to make sense, how does your approach build on/respond to what others have done, and what tools, frameworks, corpii, etc... did we use)?
% \end{itemize}

% I do my project on stitching different architectures of neural networks. Challenges are figuring out what good stitch function classes
% are as well as a lot of coding and fine-tuning. We set up our approach by creating a big experimental infrastructure to define a bunch of
% stitches across different architectures. We then use the infrastructure to create a bunch of models and compare them.

% \begin{itemize}
%    \item How does your project answer the open question?
%    \item What are the consequences of your work?
%    \item What value/contributions does your project have or offer (to broader community, to researchers, or to future work)?
% \end{itemize}

% My project gives future researches infrastructure to try and better understand and compare neural networks. The idea is that
% if we can use compare different architectures effectively, then we can use our current knowledge to understand other networks.
% It's just building bridges. My project is also going to give people language to talk about their experimental setups that makes
% it completely unambiguous what they are doing (i.e. it is easy to understand often the technicalities of some approach, but it
% is the naming which holds us back: if we can create good lingo through words like "sender" and "reciever" and other things to
% describe stitches, then the field can easily progress faster). My project overall takes us closer to answering the question by
% empowering others and giving us a space of concrete experimentation.

% NOTE: good introductions should do what we have done up here.

% This will discuss why we are doing this. The main goal is to understand neural networks, but we can think of this
% attempt to understand "modules" as a great boon to transfer learning and cheap computation since it may allow us to
% have the tools necessary to do a lot of iterative tinkering with architectures in an empirical way.
\section{Introduction}
\subsection{Motivation}
The success of deep learning for visual recognition has been attributed to the ability of neural networks to learn
good representations of their training data [Rumelhart et al., 1985]. That is, intermediate outputs of good neural networks 
are believed to encode semantic information about their inputs, which they use for classification and/or other
downstream machine learning tasks. However, our understanding of these representations is somewhat limited. We do not
know why good representations are learned, nor do we have a robust theory to characterize them. For example, we do not
know how to compare representations.

Being unable to compare representations is a fundamental limitation because it precludes us from understanding whether
different models are learning essentially the same thing. This is important, because one of our great hope is that
different neural networks are able to learn, by some measure, qualitatively similar representations. Metaphorically speaking,
we'd like it to be the case that two models' representations were the same, but in different "languages." It is as if
a ResNet18 were speaking French and a ResNet34 English, but with an effective translator we could confirm that they were
indeed speaking the same sentence.

Imagine we had such a so-called translator on hand. If our hypothesis were to be proven correct,
it would not only tell us that neural networks learn similar representations, but also lend credence to the belief that
neural networks are capable of learning representations that are human-interpretable and capture the most relevant
semantics of images. This is because, if all successful learners learn the same underlying semantics, then it
is intuitive to think that it is not mere coincidence causes them to learn those semantics.

As such, if our hypothesis were correct, it could give us more confidence in the robustness of certain architectures.
Moreover, it would enable us to tackle neural network interpretability from a new angle: instead of trying to 
understand specific networks layer by layer or neuron by neuron, we could compare different neural networks'
representations and then cluster those represnetations by similarity. 
That would then enable us to extend previous interpretability results to broader classes of neural networks.

Recent advances in model stitching [Bansal et al., 2021] have made great strides towards our goal. They use
a tool called Model Stitching to not only confirm that very similar neural networks learn similar representations, but
also improve the accuracy of models and potentially even reduce their training cost (in compute). Bansal et al.
confirm that, on standard datasets such as CIFAR-10, standard ResNets, such as ResNet18, are able to learn similar
representations irrespective of their initial random seeds, layer widths, or (to a limited extent) training methodology.
However, their research has two main holes.

Firstly, it only confirms that networks with the same fundamental architecture have similar represnetations at
the exact same layers. It is much harder to compare the representations of neural networks with different architectures
since they have different shapes, but that is precisely what we seek to ask through this paper. Do all ResNets learn
similar representations? What if one has 18 layers and another has 34? Are those 18 layers the same as the first 18
of the 34-later ResNet or are they something else?

Secondly, it neglects to address the question as to whether stitches learn the same (or approximate)
representations as the original layers, or whether they learn something else which nonetheless has the
same effect on accuracy. The perplexing results we find in our paper give some credence to the latter
possibility, but are being further investigated.

% This will discuss how stitching works on a high level. It will not introduce any mathematical formalisms or notation.
% Those will probably be used later.
\subsection{Stitching}
We broaden the definition of stitching very little from that as defined in [Bansal et al., 2021]. In that paper,
stitching involved taking the intermediate output of some layer, which we call the sender, transforming it, and then
inputting that into some intermediate input in another network. We call the layer we input into the reciever. We
will also refer to the two networks, respectively, as sender and reciever when convenient and contextually clear.
The representation that would normally be recieved by the reciever we call the expected representation (sent by
the expected sender: the previous layer in the reciever network).

The layers up to the sender (inclusive) we call a prefix and the layers from the reciever to the end (inclusive) we call
a suffix. The transformation used is called the stitch, and it is learned with gradient descent (or some other optimizer)
on a frozen prefix/suffix pair. The network formed by concatenating the sender prefix, the stitch, and the reciever
suffix, is called the stitched network. Just like Bansal et al. we use the accuracy of the stitched network as our
primary training task. (We differnetiate this later with different tasks such as the copying task: training a stitch
to transform the prefix's cached outputs into the suffix's expected inputs using a similarity metric like mean squared
error. When we use input/output representations, as opposed to images classes, as our dataset and labels we refer to 
that dataset as the representation dataset).

The stitch belongs to a simple function class such as 1x1 convolutions (depthwise) or
a linear layer. Bansal et al. these function classes since, if the sender and reciever networks are in the same
function class, the stitched network will also be in that same function class. However, since for us the sender and
reciever networks are not in the same function class, we allow stitches that modify the function class in minor ways.
We expand the class of stitches to include strided convolutions (with stride equal to kernel width, so as to downsample
by some constant factor) as well as 1x1 convolutions of nearest-upsampled layers (basically, the sender representation
is copied a constant number of times such that each pixel in it corresponds to a square grid of identical pixels).

These two additions allow us to compare all different layers of a wide class of ResNets including ResNet18, ResNet34, and all
ResNets using basic blocks with four layers (sets of blocks) with between 1 and 2 blocks within each layer. This latter class
of small Resnet, we call ResNet(x,x,x,x), where each x can be either 1 or 2 and represents how many basic blocks are in that layer
(note that ResNet18 is ResNet(2,2,2,2)).

\begin{center}
   \begin{figure}[h!]
      \centering
      \caption{Stitching}
      \includegraphics[width=6.5cm]{stitch.jpg}
      \caption*{This diagram gives us a simple view of a stitched network. The black arrows denote the flow of data, while
      the grey arrows denote the flow of data that would have been present, had the two networks been used without stitching.
      The different blocks denote different layers (numbered) and the different colors denote different neural networks. The arrow
      from layer 3 of the blue network to layer 4 of the red network involves a simple stitch transformation, though it is left
      out of the image for concision. The boxed layers, plus the stitch, make up the stitched network. In this specific case,
      the stitched network's task accuracy measures the similarity of red layer 3 with blue layer 3, since red layer 4 expects the
      input to come from red layer 3. However, we have no way of telling (given just the task accuracy) whether the representation
      from blue layer 3 to red layer 4 is the same as the representation from red layer 3 to red layer 4 or some other representation
      that yields similar results.}
   \end{figure}
\end{center}
\subsection{Figures}

% This will discuss things like CKA, CCA, Procrustes, and stitching without differnet architectures.
\subsection{Related Approaches}
Many non-learned similarity measures exist such as CKA, CCA, and the procrustes distance. These may or may not
be useful to explore the similarity of different representations. However, these measures are
plagued by the fact that they are rather ad-hoc and rely on the ingenuity of their designers [Bansal et al., 2021].
Recent research [Ding et al., 2021] has shown that supposed improvements in these measures are not true improvements,
but instead improvements in special cases. Thus, we decide not to use measure like CKA because results gotten from
their use are difficult to gague. Instead, in line with Ding et al.'s arguments, we choose a measure which is more correlated
to an accuracy, and therefore more interpretable: stitching. One of our goals, of course, is also to understand
stitching, in itself, better, so using a measure like CKA is not maximally helpful.

\subsection{Preliminary Results}
We explore the case of small ResNets on CIFAR-10, varied purely through the random seed used for weight initialization.
Our experiments include all 256 possible stitched pairs of ResNet(x,x,x,x) for x being 1 or 2 as well as ResNet18 with itself and with ResNet34
(and vice versa). For every pair we also have four controls: a random sender stitched with the reciever, a random reciever stitched
into from the sender, and a random sender stitched into a random reciever. All our experimental results are formatted as layer to
layer similarity matrices. (The row is the sender, the column is the reciever, and the compared layers are the expected sender
and the sender; the entry is the accuracy of the stitched network).

Every single pair in our experiment has the same
patterns. When two random networks are stitched, all possible stitches have low accuracy with a rather uniform distribution. When
a sender is stiched with a random reciever, only stitches from the later layers of the sender into the later layers of the (random)
reciever have high stitching accuracy. When a random sender is stitched into a reciever, only stitches into the early layers of the
reciever have high stitching accuracy. When the sender is stitched into the reciever, all layers in the lower left hand triangle
have high stitching accuracy. This intruiging last result was most apparent for small ResNets or different shapes.
Figures are below for all our results.

For ResNets of the same shape we saw more of a diagonal pattern (akin to Bansal et al.) though, the lower left hand triangle for small
resnets had higher
accuracy than the top right hand triangle. 
We are still investigating these results to understand the cause. We are considering
whether it is possible that the stitch is not learning to emulate the expected representation but instead create another representation
that yields the same results (i.e. high accuracy). To do this, we are training different stitches of the same layers on representation datasets
(that is, pretrained networks' cached intermediate outputs at different layers) instead of backpropagated CIFAR-10. The
vanilla and feature-dataset-trained stitches we will compare with mean squared error since they have the same shape.
We are also working on interpretability algorithms. Since our results are unexpected, we are in the process of reworking them to
be more effective for what we have observed.

\begin{center}
   \begin{figure}[h!]
      \centering
      \caption{Triangle Pattern in Small ResNets}
      \includegraphics[width=4.5cm]{resnet1111_1111.png}
      \includegraphics[width=4.5cm]{resnet1111_2221.png}
      \includegraphics[width=4.5cm]{resnet1112_1122.png}
      \caption*{These are similarity matrices. The row tells you the sending layer and the column tells you the recieving layer.
      The entry tells you the accuracy of the stitched model created by stitching between those two layers.}
   \end{figure}
\end{center}

\begin{center}
   \begin{figure}[h!]
      \centering
      \caption{Sometimes Triangular Pattern in Large ResNets}
      \includegraphics[width=6.5cm]{resnet18_resnet18_sims.png}
      \includegraphics[width=6.5cm]{resnet18_resnet34_sims.png}
      \caption*{These are similarity matrices. The row tells you the sending layer and the column tells you the recieving layer.
      The entry tells you the accuracy of the stitched model created by stitching between those two layers.}
   \end{figure}
\end{center}

% \section{Experimental Setup}
% % Notational notes for myself:
% %    %% Stitching %%
% % A_{i,j} is the module from i to j inclusive
% % A_{\leq N} is the prefix (i.e. module from 0 to N inclusive)
% % A_{\geq K} is the suffix of all layers after K.
% % AB_{i \rightsquigarrow | \rightarrow \not{\rightarrow} j} is the stitch from A to B from layer i to layer j
% %   depeneding on whether the accuracy is unknown, high, or low (so representations are unknown, similar, or differnet)
% % To refer to the outputs of networks we use the letter R for representation. For the output of network A
% % from layer i we use R_{A, i}. This'd be the input for network A at layer i + 1 or some sort of stitch.
% %    %% Mappings %%
% % A \rightarrow B
% % 
% % 
% \subsection{Definitions}
% We begin with a lot of definitions and then proceed to elucidate the high-level experimental setup
% and preliminary results. Our goal from the experiments is to understand what functionality is shared
% across pairs of neural network architectures. We rely heavily on a domain-specific language defined
% below because it allows us to ascribe terms with precise meaning to all the different types of behaviors
% we expect to see.

% \subsubsection*{Basic Definitions and Terminology}
% \begin{itemize}
%    \item Representation: the intermediate output of a neural network, usually refered to as \(R_{A, i}\) if output from layer \(i\) of network \(A\).
%    \item Prefix: the first \(N\) layers of a neural network, \(A\). We may call the prefix function \(A_{\leq N}\) or \(A_{<N+1}\) if applicable.
%    \item Suffix: the last layers of a neural network \(B\), starting at layer \(K\). We may call the suffix function \(B_{\geq K}\) or \(B_{>K-1}\) if applicable.
%    \item Module: a subsequence of consecutive layers in a neural network, similar to the notion of a substring in computer systems. It can also be thought of as a prefix of a suffix, or a suffix of a prefix. It is denoted with the start and ending layers (inclusive). For example \(C_{2,3}\) is the module with layers 2 and 3 of network \(C\). During normal computation, the first layer outputs into it, and it outputs into the fourth layer. Note that single layers are also modules. \(C_{1,1}\) is both the first layer and the first possible module of \(C\). Moreover, prefixes and suffixes are modules.
%    \item Submodule: a module inside a module. for example, \(C_{2,2}\) is a submodule of \(C_{2,3}\). This will be used when we wish to describe the internals of modules.
%    \item Supermodule: a module within which there is a module. This will be used when we wish to group modules.
%    \item Sender: when a module recieves input from a previous module we call that previous module the sender. We may also say that a module is \emph{sending} when we wish to express that it is the sender.
%    \item Reciever: The module some other module is sending to. We may also say that it is \emph{recieving}.
%    \item Expected sender (or reciever): absent of stitching, the module that would be the sender or the reciever to/from the current module.
%    \item Expected input (or output): the output of the expected sender or reciever during normal computation.
% \end{itemize}

% \subsubsection*{Stitching}
% We define stitching in the same way as Bansal et. al. 
% Say we wish to stitch two networks \(A\) and \(B\).
% We will calculate the output of a prefix of \(A\),
% input it into our stitch \(S\), and then input the output
% of \(S\) into a suffix of \(B\). Say we take the
% output of \(A\) at layer \(i\), input it into \(S\),
% and then take the output of \(S\) and input it into
% \(B\) at layer \(j+1\). We say that layer \(i\) of
% \(A\) was stitched \emph{into} layer \(j+1\) of \(B\).
% The composition \(S \circ A_{\leq i}\) has replaced
% the expected sender, \(B_{\leq j}\) of \(B_{>j}\).
% As discussed by Bansal et. al, this enables us to compare
% the representations \(R_{A,i}\) and \(R_{B,j}\).

% The stitch \(S\) is a function whose domain is the set of possible \(R_{A,i}\)
% and whose range is in the same space as \(R_{B,j}\). For the same reasons as Bansal et. al.
% we use simple function classes like 1x1 convolutions where applicable and strided convolutions
% elsewhere.

% The network formed with \(B_{\geq j+1} \circ S \circ A_{\leq i}\) is called the 
% \emph{stitched network} and is often denoted as \(AB_{i \rightsquigarrow j+1}\). The stitched 
% network is frozen in all layers except for \(S\), which is trained with gradient descent
% so as to approximate the best possible choice for \(S\). If the resulting accuracy
% is high then we refer to the stitched network as \(AB_{i \rightarrow j+1}\) and we say
% that \(A\) is \emph{stitchable} to \(B\) through those layers: \(\exists AB_{i \rightarrow j+1}\).
% If the resulting accuracy is low, we refer to the stitched network as \(AB_{i \not{\rightarrow} j+1}\)
% and we say that \(A\) is not stitchable to \(B\) through those layers.

% \subsubsection*{Representational Mappings}
% A representational mapping between neural networks \(A\) of length and \(B\) is a mapping that assigns
% at most one \(R_{B,j}\) for every \(R_{A,i}\). We sometimes use the layer indices \(i\) and \(j\)
% as shorthand.

% We establish the mapping as follows: for each valid \(i\) in \(A\), find the \(j\) in \(B\)
% such that \(R_{A,i}\) is most similar to \(R_{B,j}\) out of all possible \(j\) and
% \(\exists AB_{i \rightarrow j+1}\). We refer to the mapping as \(A \rightarrow B\). There are
% other ways to define such a relation, but we pick this one for simplicity.
% Intuitively this says that those two representations are, to an extent, interchangeable.
% We also hope that this mapping will be injective and monotonic.
% By monotonic we simply mean that if \(i_{1} < i_{2}\), \(i_{1}\) maps to \(j_{1}\),
% and \(i_{2}\) maps to \(j_{2}\), then it should be the case that \(j_{1} < j_{2}\). This simply
% means that the network is learning more abstract notions as data flows through it.

% The precise reader will note that while the representations may be interchangeable in one direction
% they may not be in both, depending on the type of stitch. That is to say,  \(\exists AB_{i \rightarrow j+1}\)
% does not imply \(\exists BA_{j \rightarrow i+1}\), even with linear transformations and 1x1 convolutions
% because of the possibility of drastic changes in dimensionality. We use \(\rightarrow\) and \(\leftarrow\)
% or flip the order of the operands to differentiate the two directions.
% When we wish to express \(\exists AB_{i \rightarrow j+1} \land \exists BA_{j \rightarrow i+1}\),
% we may often say \(AB_{i \leftrightarrow j}\). We may also define a two-way representational mapping,
% \(A \leftrightarrow B = (A \rightarrow B) \cap (B \rightarrow A)\), though it is not a mapping
% in the mathematical sense, but instead a list of tuples.
% % Begin Footnote
% \footnote{For a set of tuples \(C\), Define the reverse tuple set \(V(C) = \{(b, a) \forall (a, b) \in C\}\).
% For a mapping \(f: D \to R\) we define its set of tuples, \(T(f) = \{(x, f(x)) \forall x \in D\}\).
% For \(f: D \to R, g: R \to D\), we define \(f \cap g = T(f) \cap V(T(g))\). We can also define
% \(f \cup g = T(f) \cup V(T(g))\). Note that \(f \cap g\) is always a subset of \(D x R\), and
% that \(f \cap g = V(g \cap f)\)}.
% % End Footnote

% The precise reader will also note that high accuracy should be relative to some baseline. We define what is high
% experimentally and using random (untrained) networks. When stitching different architectures we compare the accuracy
% with that of the \emph{least accurate} architecture that went into the stitch. That means that if we stitch from
% a very large (or wide) network to a very small one, or vice versa, we should expect the smaller network to be a
% bottleneck on accuracy.

% \subsubsection*{Modular Mapping}
% Using our representational mappings, we establish a more general notion of modular mappings based on modules. A Modular mapping
% tells us about what modules in two different neural networks might have shared functionality based on representational similarity.
% Intuitively, if one module is mapped to another, then the latter can be swapped out for the former, with two stitches, in the latter's network.
% Informally, for two networks \(A\) and \(B\), \(A_{i_1, i_2}\) is mapped to \(B_{j_1, j_2}\) if we can find a pair of stitches \(S_1, S_2\), such that the accuracy is high for
% \(B_{>j_2} \circ S_2 \circ A_{i_1,i_2} \circ S_1 \circ B_{<j_1}\). Since it's possible to infer this from \(A \rightarrow B\) and \(B \rightarrow A\),
% however, we infer it from \(\exists BA_{j_1 \rightarrow i_1}\) and \(\exists AB_{i_2 \rightarrow j_2+1}\).

% To find these modular mappings we can simply brute force for all possible indices, \(i_1, i_2, j_1, j_2\),
% given our pre-computed representational mappings. If we denote the length (in layers) of \(A\) as \(N_1\) and
% that of \(B\) as \(N_2\), The runtime of the brute force algorithm is \(\mathcal{O}(\alpha N_1 N_2)\) where \(\alpha\) is
% the worst-case time necessary to optimize a stitch between any pair of layers.

% However, we would prefer to also try and enforce monotonicity of the modular mapping. If two modules, \(M_1 = A_{i_1, i_2}, M_2 = A_{i_3, i_4}\),
% have the property that \(i_2 < i_3\), we say that \(M_1 < M_2\). Monotonicity means that of we have any two modules in \(A\),
% \(M_{A_1}, M_{A_2}\), and any two modules in \(B\), \(M_{B_1}, M_{B_2}\), such that \(M_{A_1} \Rightarrow M_{B_1}\)
% and \(M_{A_2} \Rightarrow M_{B_2}\), then \(M_{A_1} < M_{A_2}\)implies \(M_{B_1} < M_{B_2}\). Monotonicity is preferable, because
% it makes the mapping more interpretable. It may not be the case that the modular mapping is monotonic, but we can instead choose to
% focus on a subset of module pairs that is. We present an algorithm below which given two neural networks finds a monotonic subset of
% \(A \Rightarrow B\).

% \begin{center}
% \begin{algorithm}
% \caption{Find A Monotonic Modular Mapping Subset}
% \begin{algorithmic}[h]
% \Procedure{MFind(\(A\), \(B\))}{}
%       \State Pretrain and Freeze \(A\) and \(B\) seperately
%       \State \(A\) has length \(N\)
%       \State \(B\) has length \(M\)
%       \State Find \(A\rightarrow B\) through brute force
%       \State Find \(B\rightarrow A\) through brute force
%       \State \(i_{start} = 1\)
%       \State \(j_{start} = 1\)
%       \State Initialize \(MM = \{\}\)
%       \For{\(i_{end} \in [1, N-1]\)}
%          \For{\(j_{end} \in [j_{start}, M-1]\)}
%             \If{\((B_{j_{start}-1} \rightarrow A_{i_{start}}) \land (A_{i_{end}} \rightarrow B_{j_{end}+1})\)}
%                \State Add \((A_{i_{start}, i_{end}}, B_{j_{start}, j_{end}})\) to \(MM\)
%                \State \(i_{start} = i_{end} + 1\)
%                \State \(j_{start} = j_{end} + 1\)
%                \State Break Inner Loop
%             \EndIf
%          \EndFor
%       \EndFor
%       \State Return \(MM\)
% \EndProcedure
% \end{algorithmic}
% \end{algorithm}
% \end{center}

% \subsection{Hypotheses}
% Now that we know how to talk about modules, we introduce 4 main idealized cases that may present themselves when we
% try to modularly map two different neural networks of different sizes. We assume that these mappings are all monotonic.
% We describe these assuming that the sender network is at most as long as the reciever network.

% \begin{itemize}
%    \item Layer to layer (L2L) mapping: single layers (in shorter networks) may have the same functionality as single layers in longer networks. Additional layers in longer networks introduce new functionality. This mapping is injective, but not necessarily surjective.
%    \item Layer to module (L2M) mapping: single layers (in shorter networks) may have the same functionality as modules in longer networks. The long networks have the same functionality, but better. This mapping is injective and ideally surjective.
%    \item Module to module (M2M) mapping: modules (in shorter networks) may have the same functionality as modules in longer networks. This mapping is injective and ideally surjective.
%    \item No (N) mapping: informally, when there is M2M mapping, but the modules are so big as to be uninformative.
% \end{itemize}

% It is possible that our mappings will not be monotonic, but we hope to find that they are. It is an elegant
% simplifiying assumption.

% Given this assumption, it is (only) possible to have these four cases outlined above. However, which one
% is most prevalent at a specific segment of a network may change: we may start with L2L mappings followed by
% M2M or N mappings, for example. To define a notion of this, we introduce \textit{realms}. Realms are pairs
% of supermodules within which submodules obey a single mapping mode (L2L, L2M, M2M, or N).
% More generally, if within a pair of supermodules behavior follows some pattern, we label it a realm.

% Thus, we may re-pharse our goal as that of describing the realms of modular mappings for different
% neural network architectures. To do this we generate a monotonic modular mapping based on representational
% mappings which we found using stitching. This helps us understand for pairs of networks, where they have the
% same functionality in a precise way.

% \subsection{Networks and Dataset}
% We focused on varying only the random seed of the neural network. To simplify our analysis we also used the CIFAR-10 dataset
% with standard resnets. These currently only include ResNet18, but will soon also include  ResNet34, ResNet50, ResNet101, ResNet152.
% The optimizers and hyperparameters used are further elucidated in the appendix.

% For layers with the same width and height we used 1x1 convolutions as our stitching function class. For layers with different dimensions
% we utilized the fact that the dimensions of the layers are powers of two (in our networks). In stitches that required downsampling (due
% to smaller dimensions in the reciever) we used strided convolutions. In stitches that required upsampling we simply used a nearest
% upsampling followed by a 1x1 convolution (which you can think of as reversed strided convolutions).

% \subsection{Tests and Controls}
% \subsubsection*{Networks of the same architecture}
% First we stitched networks of each architecture with other networks of the same architecture. We trained lists of networks for each 
% architecture. For each list we called the first element the \textit{pivot}. The pivot was stitched to each of the other networks
% in the same list, including itself. For every pair of networks we stitched each layer in the sender network to each layer in the
% reciever network. 

% For the purpose of creating a control, we made sure that there was a random (untrained) reciever network in the list. We also swapped 
% pivots with another untrained network which was stitched with all the other networks in the list. This ensured that we had baselines
% for resulting accuracy (and therefore similarity).

% You can think of the result of each pair of networks' stitches as a square matrix of accuracies, where the row denotes the layer of the
% sender network and the column denotes the layer of the reciever network. The accuracy at that elements is our measure of representational
% similarity. These tables are tiled into a tensor which represents the results of the stitch. In the depth dimension we average to find
% the average similarity over all our experiments for that architecture. Note that we insert zero at layer pairs that are not stichable
% due to dissimilar shapes.

% \subsubsection*{Networks of different architectures}
% We will conduct the same experiment as above, but where the pivot (sender) architecture is different from the reciever architecture.
% For each pair of architectures, we pick a pivot of each architecture along with a (small) list of recievers of the other architecture.
% We then repeated the procedure from the case with two identical architectures. Note that instead of a square matrix of accuracies, we
% have a rectangular matrix of accuracies. This matrix will be tall if the sender network is longer than the reciever network, and wide
% in the other case.

% \subsection{Testing Algorithm}
% More details will be elucidated in the appendix.

% \section{Results}
% \subsection{Summary}
% We have preliminary results that suggest the expected: networks of the same architecture yield a high similarity
% along the layer-to-layer matrix diagonal, with decreasing accuracies towards the top right and bottom left corners. This tells us
% that they are in a L2L realm (where corresponding layers have similar output representations and therefore functionality).
% For a random network, we have random behavior. We trained on a ResNet18 and have graphics below.
% \begin{center}
%    \includegraphics[width=6.5cm]{sim.png}
%    \includegraphics[width=6.5cm]{rand.png}
% \end{center}
% \subsection{Figures}
% ...

% % Figures that make sense when they are in black and white are ideal
% % \begin{figure}[h]
% % \begin{center}
% % %\framebox[4.0in]{$\;$}
% % \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
% % \end{center}
% % \caption{Sample figure caption.}
% % \end{figure}

% \subsection{Tables}
% ...

% % \begin{table}[h]
% % \caption{Sample table title}
% % \label{sample-table}
% % \begin{center}
% % \begin{tabular}{ll}
% % \multicolumn{1}{c}{\bf PART}  &\multicolumn{1}{c}{\bf DESCRIPTION}
% % \\ \hline \\
% % Dendrite         &Input terminal \\
% % Axon             &Output terminal \\
% % Soma             &Cell body (contains cell nucleus) \\
% % \end{tabular}
% % \end{center}
% % \end{table}

% \section{Analysis and Interpretation of Results}
% \subsection{Interpretation}
% ...
% % It didn't work.

% \subsection{Significance}
% ...
% % What this means for the future and the field.

% \section{Conclusions}
% ...
% % We'll write our conclusions in a nice format here.

% \section*{Acknowledgments}
% Thank you to the SuperUROP benefactors (MIT EECS) for funding this project.

% \bibliography{iclr2021_conference}
% \bibliographystyle{iclr2021_conference}

% \appendix
% \section{Appendix}
% ...

\end{document}
